<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Brendan Malone</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="/feed.xml" />
<link rel="alternate" type="text/html" href="" />
<updated>2017-01-11T20:56:52+11:00</updated>
<id>/</id>
<author>
  <name>Brendan Malone</name>
  <uri>/</uri>
  <email>malone.brendan1001@gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[Journal Paper Digests]]></title>
 <link rel="alternate" type="text/html" href="/2017/01/journalDigest" />
  <id>/2017/01/journalDigest</id>
  <updated>2017-01-09T00:00:00-00:00</updated>
  <published>2017-01-09T00:00:00+11:00</published>
  
  <author>
    <name>Brendan Malone</name>
    <uri></uri>
    <email>malone.brendan1001@gmail.com</email>
  </author>
  <content type="html">
    &lt;h2 id=&quot;journal-paper-digests-2017-1&quot;&gt;Journal Paper Digests 2017 #1&lt;/h2&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;stability-of-peatland-carbon-to-rising-temperatures&quot;&gt;Stability of peatland carbon to rising temperatures&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://www.nature.com/articles/ncomms13723&quot;&gt;Link&lt;/a&gt; to paper.&lt;/p&gt;

&lt;p&gt;Abstract:
Peatlands contain one-third of soil carbon (C), mostly buried in deep,
saturated anoxic zones (catotelm). The response of catotelm C to climate
forcing is uncertain, because prior experiments have focused on surface
warming. We show that deep peat heating of a 2 m-thick peat column
results in an exponential increase in CH4 emissions. However, this
response is due solely to surface processes and not degradation of
catotelm peat. Incubations show that only the top 20-30 cm of peat from
experimental plots have higher CH4 production rates at elevated
temperatures. Radiocarbon analyses demonstrate that CH4 and CO2 are
produced primarily from decomposition of surface-derived modern
photosynthate, not catotelm C. There are no differences in microbial
abundances, dissolved organic matter concentrations or degradative
enzyme activities among treatments. These results suggest that although
surface peat will respond to increasing temperature, the large reservoir
of catotelm C is stable under current anoxic conditions.&lt;/p&gt;

&lt;h3 id=&quot;carbon-accounting&quot;&gt;Carbon accounting&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://www.nature.com/nclimate/journal/v6/n11/full/nclimate3137.html&quot;&gt;Link&lt;/a&gt; to paper.&lt;/p&gt;

&lt;p&gt;Atmospheric CO2 concentrations are at the highest level for around 15 million years. Accurate accounting is crucial for informed decision-making on how to curb the rise.&lt;/p&gt;

&lt;h3 id=&quot;molecular-fingerprint-of-soil-organic-matter-as-an-indicator-of-pedogenesis-processes-in-technosols&quot;&gt;Molecular fingerprint of soil organic matter as an indicator of pedogenesis processes in Technosols&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://link.springer.com/article/10.1007/s11368-016-1523-1&quot;&gt;Link&lt;/a&gt; to paper&lt;/p&gt;

&lt;p&gt;Abstract:
Technosol management is one of the greatest challenges for the future,
more specifically as regards supporting and/or restoring ecosystems. The
understanding of natural soil organic matter (SOM) dynamic from
Technosol may give important information about soil functioning and
Technosol evolution.According to this, SOM from three French old mine
Technosols, (an old tin mine, a lead and zinc, and a gold one which is
arsenic-rich), were studied and characterized using thermochemolysis
coupled with gas chromatography and mass spectrometry (GC-MS) with
tetramethyl ammonium hydroxide (TMAH) as reagent and FTIR. The
characterization and quantification of some specific biomacromolecules,
used as biomarkers, indicate the specific level of incorporation
relative to various subgroups. Global parameters of soils (pH, total
organic matter, cation exchange capacityaEuro broken vertical bar) were
also evaluated.Results on bulk samples show that lipids are the most
reactive group and therefore play the most important role in young soil
pedogenesis. All of the results show that the behavior of SOM of the
Technosol is similar to homolog non-anthropized soil and depends on
vegetation type.A slight inhibition of bacterial activity is observed
which underlines a protective effect of Technosols on SOM degradation
due to the low pH, the high clay content, and the presence of Al3+ and
metal(loid)s. In fine, lipid fraction of SOM may act as a well-done
fingerprint of pedogenesis processes in Technosols.&lt;/p&gt;

&lt;h3 id=&quot;moisture-stress-indicators-in-giant-sequoia-groves-in-the-southern-sierra-nevada-of-california-usa&quot;&gt;Moisture Stress Indicators in Giant Sequoia Groves in the Southern Sierra Nevada of California, USA&lt;/h3&gt;

&lt;p&gt;Authors:
Ray, RL&lt;/p&gt;

&lt;p&gt;Source:
&lt;em&gt;VADOSE ZONE JOURNAL&lt;/em&gt;, 15 (10):NIL_15-NIL_33; OCT 2016&lt;/p&gt;

&lt;p&gt;Giant sequoia [Sequoiadendron giganteum (Lindl.) J. Buchholz] trees and
their ecosystems are unique natural treasures in the Sierra Nevada,
California, where most groves are federally managed for biodiversity,
perpetuation of the species, and aesthetic, recreational, ecological,
and scientific values. Increasing temperatures during the next several
decades may create conditions unfavorable for these giant sequoias.
Therefore, it is necessary to develop effective management systems to
preserve the health of these giant sequoia groves. This study used a
topographic wetness index (TWI) as the indicator of soil moisture
conditions to evaluate the vulnerability of giant sequoia groves to soil
moisture stress and focused on evaluating TWI distributions among all 70
sequoia groves to assess their vulnerability to soil moisture stress.
The TWI values were derived using a 10-m digital elevation model and
compared with soil, geology, slope, aspect, and elevation at the sequoia
groves to understand the vulnerability of the groves to soil moisture
stress. The TWI values were also compared with snow cover persistence
derived from 12 yr of MODIS snow cover products. In addition, satellite
soil moisture products were used to compare the dry and wet periods
predicted by snow cover persistence. Results showed that the groves
located at higher elevation are less vulnerable unless the TWI across
the groves is low. For the large number of groves with elevations mainly
in the 1800-to 2100-m range, the TWI distributions can serve as a
first-order indicator of relative vulnerability. Further, this analysis
showed that areas with milder slopes and more converging area (higher
TWI), plus longer snow cover persistence, should be less susceptible to
low summer soil moisture than areas having steeper slopes, more
diverging topography (lower TWI), and earlier snowmelt. This analysis
can be used to highlight groves that are potentially more vulnerable,
particularly when considering TWI, snow cover persistence, and satellite
soil moisture together.&lt;/p&gt;

&lt;h3 id=&quot;environmental-change-and-human-health-can-environmental-proxies-inform-the-biodiversity-hypothesis-for-protective-microbial-human-contact&quot;&gt;Environmental Change and Human Health: Can Environmental Proxies Inform the Biodiversity Hypothesis for Protective Microbial-Human Contact?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://academic.oup.com/bioscience/article/66/12/1023/2646819/Environmental-Change-and-Human-Health-Can&quot;&gt;Link&lt;/a&gt; to paper.&lt;/p&gt;

&lt;p&gt;Authors:
Liddicoat, C; Waycott, M; Weinstein, P&lt;/p&gt;

&lt;p&gt;Source:
&lt;em&gt;BIOSCIENCE&lt;/em&gt;, 66 (12):1023-1034; DEC 2016&lt;/p&gt;

&lt;p&gt;Abstract:
Microbiota from environmental sources overlap and interact with human
microbiota, contribute to human microbial diversity, and provide
beneficial immunomodulatory stimuli. Meanwhile, reduced diversity in
human microbiota and immune dysregulation have been associated with a
range of diseases. Emerging evidence suggests landscape-scale drivers of
microbial diversity may influence our health, but the area remains
understudied because of its multidisciplinary nature. Here, we attempt
to widen the view on this subject by offering an environmental
researcher’s viewpoint, proposing a unifying conceptual framework to
stimulate multidisciplinary interest. To focus research in this
challenging area, we propose greater emphasis on multiscale ecological
links and that landscape-scale proxies for potential underlying
microbial mechanisms be investigated to identify key environmental
attributes and health relationships worthy of subsequent detailed
examination. Wherever possible, ecological epidemiological studies
should account for the temporal nature of environmental microbiota
exposures, especially with respect to the early development of the human
commensal microbiota.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;/2017/01/journalDigest&quot;&gt;Journal Paper Digests&lt;/a&gt; was originally published by Brendan Malone at &lt;a href=&quot;&quot;&gt;Brendan Malone&lt;/a&gt; on January 09, 2017.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Soil Science Conference, Queenstown, New Zealand]]></title>
 <link rel="alternate" type="text/html" href="/2016/12/queenstown" />
  <id>/2016/12/queenstown</id>
  <updated>2016-12-22T00:00:00-00:00</updated>
  <published>2016-12-22T00:00:00+11:00</published>
  
  <author>
    <name>Brendan Malone</name>
    <uri></uri>
    <email>malone.brendan1001@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;From 12-16 December 2016, I had the pleasure of attending the joint conference of the New Zealand Society of Soil Science and Soil Science Australia. The conference was held in one of the most iconic places of New Zealand: the scenic resort town of Queenstown in the South Island. The overarching theme of the conference was &lt;a href=&quot;http://www.nzsssconference.co.nz/&quot;&gt;‘Soil, a Balancing Act Down-under’&lt;/a&gt; to glance the torch light onto the challenges in managing soils to reach a careful balance between many, often competing, land-use, productivity and environmental aspirations. I report on some of my observations from this interesting conference.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;The general schedule of the conference was fieldtrips on day 1, followed by days 2, 3, 4, and half of 5 as normal conference proceedings consisting of keynotes, plenaries, and research presentations (both oral and posters). Some social functions were organised which included a BBQ at the end of day 1, a touch football competition (day 3), and conference dinner (day 4). The conference was situated at the Millennium Hotel, which is more-or-less central Queenstown. I arrived in New Zealand of the 11th December with a few colleagues, and departed for return to Sydney just after lunch on the 16th.&lt;/p&gt;

&lt;h2 id=&quot;conference-fieldtrip&quot;&gt;1. Conference fieldtrip&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;First day of the conference was dedicated to a fieldtrip. There were two options for fieldtrips, and I went on the one that went through the Central Otago area. Most of my work colleagues were on this trip too.&lt;/p&gt;

&lt;p&gt;Essentially the trip was a loop of the area to the east and north east of Queenstown. Pretty much after departing Queenstown and heading directly east we headed to the Crown Range Road Summit which is at 1119m elevation. The route to the summit was very steep and winding. Upon disembarking from the bus we were greeted with exceedingly chilly winds and the odd flake of snow. We were also greeted by tour leader Sam Carrick (Landcare Research) who introduced us to a freshly prepared Acidic Brown Soil - largely a silty soil with an abundance of Schist rocks of various size. This particular landscape is an example of New Zealand’s high country of which is mainly dedicated to pastoral farming. Tourism is also big in this type of landscape too. The combined pressures of farming and tourism makes management of these delicate landscapes a tricky task as they are both culturally and environmentally significant, yet also are a source of significant dollars in terms of the pastoral outputs and their tourism potential. Similar mindful approaches the land management are common elsewhere where competing and multipurpose land uses are in play. After spending around 20mins at this stop, most people were clamouring to get back onto the buses for the next item of the itinerary due to the near freezing temperature.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/20161222queenstown/highcountry1.jpg&quot; alt=&quot;high country figure&quot; /&gt;
    &lt;figcaption&gt;Figure 1. An example of New Zealand's high country. Complete with grazing sheep and snow capped mountains.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The second stop was Mt Burke Station which is farmed by Tim Burdon with farm manager Grant Ruddenklau. Mt Burke Station covers approx. 10 000ha and is situated on the shores of Lake Wanaka. There are mixed land uses here with significant numbers of stock that include sheep, cattle (beef), and deer. Some cropping enterprises include Canola, Swedes, Turnips, and fodder beet. We were here to discuss in person some of the land management issues facing farming in high country pastoral stations. Mt Burke is typical of many high country stations in the lakes catchments. It has hundreds of streams flowing through the property and shallow stony soils that present challenges for water quality and nutrient management, and it also has the vast challenge of having an open lake frontage. Having this lake frontage (Lake Wanaka) leads to higher scrutiny from Government in how the land is managed, and how and to what extent nutrients are used and applied. In fact very stringent monitoring and reporting of nutrient use is the norm on this station (and elsewhere). Also having the lake frontage exposes the farm managers to greater scrutiny from the public who are naturally concerned and oftentimes vocal about the maintenance of a pristine environment. According to the farm managers, sometimes this scrutiny is mis-guided, leading to negative perceptions of agricultural land management in this delicate landscape. From what I could tell though, with government policy in place and stringent monitoring, coupled with a team of knowledgeable land managers, Mt Burke station is doing well and operating well. After having lunch on the shore of Lake Wanaka, Sam Carrick showed us a nice example of soils on this station, where we were introduced to a stunning Stoney Brown soil. There are so many knowledge gaps in understanding these soils, particularly how they respond to the intensification of agriculture. Part of this knowledge gap is trying to understand how nutrients are stored and flow through them, and measuring their water storage capacity.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/20161222queenstown/mtBurke1.jpg&quot; alt=&quot;Mt Burke Station&quot; /&gt;
    &lt;figcaption&gt;Figure 2. Left image shows Sam Carrick and others talking about this very interesting Stony Brown Soil profile. The right image is a close-up of the same profile.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The third stop before journeying back to Queenstown was to meet local viticulturist Roger Gibson to talk about horticulture in the central Otago environment. We were specifically located in the Cromwell area which is situated about 50kms due east of Queenstown. What really struck me about this area was how dry the environment was. In fact the area is classified as semi-arid where it receives less than 500mm of rainfall annually. The cool environment coupled with low rainfall, and pretty good soils makes this area quite a famous viticultural region. Roger Gibson manages the award winning Lowburn Ferry wine brand and gave us an excellent talk about the history of the wine industry in this region, and the associated management considerations for producing high quality wines. We were also treated to a delicious Otago Pinot Noir while listening to Roger and taking in the astounding surrounding landscape. We then got the opportunity to meet another great soil which reminded me of something I would see in Australia. This was an apparently weathered soil with a clay pan and a clear presence of pedogentic carbonate materials. The source of pedogenic carbonates is debated, but is probably due to the combination of mineral weathering and microbial action. Coupled with the semi-arid environment, leaching of carbonates would be minimal, leading to an accumulation and precipitation within the soil profile. Sam Carrick mentioned that from dating experiments upon these soils, it is estimated to be around 400 000 years old. Compared to Australian soils, this is relatively young, but by New Zealand standards these are quite old soils. I was fascinated by this stop because I did not expect to experience a semi-arid environment in this part of the world, and to come upon a soil that was so apparently weathered.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/20161222queenstown/horticulture1.jpg&quot; alt=&quot;Lowburn Ferry&quot; /&gt;
    &lt;figcaption&gt;Figure 3. Roger Gibson gave an excellent presentation about the wine industry in the semi-arid Central Otago region. The landscape we were situated in was simply breath-taking. The soil profile we view here displayed an apparently clayey sub soil and a pedogenic calcium carbonate horizon too.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In summing up, the Otago fieldtrip was a memorable one. I was beguiled by the landscape and terrain we passed through from stop to stop. I was equally impressed by the variety of soils we experienced too. Sam Carrick’s knowledge of the environment and the issues faced with managing it was extensive and augmented the day very nicely. After gettting back into Queenstown in the early evening, everyone headed to the pre-conference BBQ where everyone got a chance to catch up and put away a tasty feed and a few quenching ales. The conference BBQ was also where the winners of the soil judging contest were announced. The gong was taken out by a travelling USA team. Second place was taken out by a team from Queensland, and the third place was taken out by our very own University of Sydney Team. Or thats how i interpretted the outcome as it seemed every team got an award somehow.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/20161222queenstown/horticulture2.jpg&quot; alt=&quot;Lowburn Ferry 2&quot; /&gt;
    &lt;figcaption&gt;Figure 4. Quite a gathering of Sydney Uni soil scientists attended the Central Otago fieldtrip.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;normal-conference-proceedings&quot;&gt;2. Normal conference proceedings&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;The format for the four days of conference proceedings were morning keynotes and plenaries to the whole delegation. This was followed by four parallel sessions. I attended a diversity of talks.&lt;/p&gt;

&lt;p&gt;The afternoon of the first day I spent most of my time in the pedology, soil landscapes and mapping session. Assoc Prof. Stephen Cattle expertly chaired the first session of the topic, and I was greatly thrilled by diversity of talks. Talks ranged from traditional pedological discussions through to digital soil mapping and informatic presentations. For example from Richard Greene (ANU) about the characteristics of Aeolian accession on soils in the Yass Valley, NSW. Then we were introduced to some innovative digital soil mapping approaches to soil carbon (Sanjee Dwage) and soil moisture (Tom Bishop). Uta Stockmann and Pierre Roudier provided some interesting presentations about use infrared spectroscopy for estimation of soil properties. Pierre’s work on digital morphometric analysis of soil profile imagery was exceptional. I really think quality imagery of soil profiles could be considered an art form. Pierre’s work demonstrated some tools for visualisation and quantitative analysis of these images.&lt;/p&gt;

&lt;p&gt;Late in the afternoon I attended the session called Improved Decision Making. A colleague of mine Ed Jones was presenting in this session where he was elaborating on the design of a soil spectral inference system. In this session I was also fascinated to see the talk from the NSW Govt (Greg Chapman) regarding soil capability prediction throughout the state. I was encouraged to see some adoption of digital technologies in their process. Being in this session from go to end meant I was unable to attend to parallel session on soil carbon where there looked to be some really interesting talks. But at least I have the full handbook of abstracts of all talks presented at this &lt;a href=&quot;http://www.nzsssconference.co.nz/images/oral_abstracts.pdf&quot;&gt;conference&lt;/a&gt; to read up on later.&lt;/p&gt;

&lt;p&gt;The morning of second day I spent in the second session of the pedology, soil landscapes and mapping session. Colleague Philip Hughes gave a lively presentation about comparing taxonomic similarities between the Australian and USA soil classification systems. I think this type of analysis is critical if the soil science domain is to offer up to the general science community a universal classification system, rather than fragmented systems established by individual organisations and countries. My other colleague Stephen Cattle presented something of a pet topic for him on whether ‘parna’ - dust derived clay aggregations - existed. Using some micromorphological analyses, Stephen was able to demonstrate that silt-size aggregations of clay did exist in soils where Aeolian accession had occurred upon them. This talk was followed up by Carol Smith (Lincoln University) who gave us an excellent discussion of Aeolian derived soils in New Zealand. I was really fascinated by her use of the &lt;a href=&quot;http://www.csiro.au/en/Research/MRF/Areas/Mineral-exploration-and-discovery/Rapid-resource-characterisation/QEMSCAN?ref=/CSIRO/Website/Research/MRF/Areas/Advanced-characterisation-facility/QEMSCAN&quot;&gt;QEMSCAN&lt;/a&gt; for discrimination of minerals within an albeit small sample. QEMSCAN employs a scanning electron microscope, four X-ray detectors and a software package to enables rapid discrimination of minerals, without reliance on visual judgments. This technology is something I should follow up on as it seems pretty useful. Other talks in this session ranged from digital soil mapping (Sharn Hainsworth, Jon Gray), to farm scale soil mapping - where Alan Palmer highlighted the inefficiencies of applying legacy soil survey mapping for farm scale applications. Marta Camps Arbestian gave a really nice presentation of soil geochemistry and its use in devising a new model of soil formation (need to read the abstract on this one).&lt;/p&gt;

&lt;p&gt;A benefit of the parallel sessions being in such close proximity to each other, meant it was pretty easy to jump from session to session. This is what I did during the second afternoon where I jumped between the 3 sessions on offer: Laboratory and Rapid Testing (Galaxy 1), session 3 of pedology (Galaxy 2), and Balanced solutions farm systems and catchments (Galaxy 3). Again the breadth of presentations in the pedology session was remarkable. Bernard Walker (UTAS) presented some of his Honours research about assessing the likely provenance of E Horizons on some soils in Tasmania. For such a fresh young researcher, this was really quality research and more-or-less very thorough detective work. Colleague Stacey Paterson gave a nice presentation on soil variability analysis of soil texture at the continental scale (Australia) using a geostatistical and fractal approach. Then there were a couple of talks from QLD Govt, where I was encouraged by their application of digital approaches for soil mapping. Lauren O’Brien presented some excellent work using disaggregation methods, similar to the &lt;a href=&quot;/software/&quot;&gt;DSMART&lt;/a&gt; approach, for identifying subsoil constraints. USYD Alumni, Luke Finn also presented in digital soil erodibility mapping in the QLD Fitzroy basin. In Galaxy 3, Darren Kidd presented on a practical approach to quantifying soil security. As a concept, soil security works well for highlighting the importance of soil, and for communicating soil science to the non-soil science community. Trying to quantify the different dimensions of soil security through a digital soil mapping approach is pretty ambitious, but I think Darren communicated quite well the different things that were and could be considered. For example, how do we quantify the cultural value of soils? With some ingenuity and accessing what appear to be excellent GIS facilities within the Tasmanian Govt, they were able to link tourism activity to cultural value. I acknowledge that Darren’s work was a first step in the quantification thing, but at a least it got the cogs turning in how we can develop the concept further and deeper. The last two talks in Galaxy 1 were of interest to me. Les Janik (CSIRO) gave an excellent presentation on the comparative performance of different portable infrared sensors for predicting soil properties in the field. I was really encouraged to hear that the Spectral Evolution vis-NIR was one if not the best performing of the different instruments. Colleague Mario Fajardo pretty much floored the room with an excellent presentation on an App he created that measures soil aggregate stability. The amount of tech savviness and code underlying this App will probably never be appreciated, but this sort of technology gives a glimpse on where soil measurement approaches are headed in the coming years. For his efforts he was ultimately awarded the Bouma Award for best presentation of the conference. Ripper stuff Mario!&lt;/p&gt;

&lt;p&gt;Day three meant it was now time for me give my own presentation on work that I had been doing in the past year in terms of spatial downscaling for soil carbon auditing. But before this I attended the Keynote and Plenary session first up in the morning. Without commenting too much about the talk from Julian Cribb, I am always upset by people presenting doomsday predictions of the world future. Julian expertly does this, but I do not think it is entirely useful, and if anything, may cause undue alarm. Sure humans have done severe damage to the earth system, but I think there is a resilient and adaptive quality to these that are often not acknowledged or under appreciated. I acknowledge human behaviour needs to change in order to better manage environmental systems, but it all comes down to education and not frightening the bejesus out of people about predictions of where we are headed. That is my little rant over, but my mood lifted significantly with the following three Plenary talks. A nice heart-warming and inspiring presentation from Alison Collins (Landcare Research) talk about the importance of soil and keeping an eye on it for the future. I liked how Alison used imagery of her young family to highlight that how we manage soils now is of great importance for future generations. Francis Hoyle (UWA) gave us a challenging talk on the dynamic changes in soil carbon. Using research conducted by her group in West Australia, sequestering soil carbon is not a simple cut and dry approach. One needs to consider application rates of amendments in addition to the economic costs of application. Soil carbon sequestration seems possible with the right management practices, but they need to be nuanced and implemented over of a long period of time, are what I think was the ultimate message. Filling in for Sam Carrick, Alan Hewitt (Landcare Research) provided a presentation on the special soil wonders of the Otago basins. This talk augmented really well with what I experienced in the field during the Otago fieldtrip.&lt;/p&gt;

&lt;p&gt;My presentation was allocated to the second session of the Improved Decision making session. I was amongst some good company including Peter Wilson (CSIRO) talking about soil data standards of which are necessary for harmonising disparate soil data sets. Peter Dahlhaus added to this concept with work in Victoria using soil data visualisations and analysis for monitoring the condition of soils. Colleague and PhD student Ronald Muchelo highlighted to the audience in his presentation how urban expansion in Uganda is leading to substantial losses of viable agricultural soils. The encroachment of urban development upon agricultural land is very familiar, particularly in Sydney, but it seems to be on a whole different level in Uganda though. Ronald in his talk demonstrated some the analyses he used for quantifying soils of which were based on digital soil mapping approaches of which I found to be really encouraging to see it being used for operational purposes.&lt;/p&gt;

&lt;p&gt;With other prior commitments, I was unable to attend the rest of the Thursday conference proceedings. People whom I spoke with after though were really impressed with the panel discussion that occurred. Another notable conference functions that I have not detailed yet is the session dedicated to poster presentations. The poster session was on after lunch on the Wednesday, and provided an opportunity to walk about and look at all the posters of which there were many. Because of the arrangement of the venue, there were posters in all nooks and crannies about the place. This made getting to all posters a little difficult, but the experience was quite enjoyable and I was able to get round to most and have a few good discussions and catch-up moments with a number of people.&lt;/p&gt;

&lt;p&gt;The last day of the conference was scheduled as a half-dayer. I admired the people who had to get up and present as I am sure there would have been a few sore heads amongst them, due to the raucous night that was the conference dinner. Highlights were presentations from colleagues and PhD students Patrick Filippi and Liana Johnson. Patrick presented on monitoring soil change in cotton field of western NSW, while Liana gave a nice presentation about heavy metal contamination of soils in the Sydney Basin. After the research presentations were completed the formalities of the conference were over. Just before this however, the Norman Taylor Memorial Lecture was given by Louis Schipper. Norman Taylor has left a huge legacy upon soil science in New Zealand from what I have read, and I think Louis, more-or-less giving a rundown of the research that comes out of the group at the University of Waikato, gave an excellent taste of soil science research in New Zealand. I was fascinated to hear about their research on soil carbon and nitrogen dynamics, of which has inspired me to follow up more deeply on their excellent research in this field. With the lecture over, the official closing ceremony happened, which included some award giving and awkward thank you and well wishes from committee members of the New Zealand and Australian soil science societies. All in all, I got a lot out the formal part of the conference. It is unfortunate I was unable to catch all that was presented, but I walk away satisfied with what I had taken in.&lt;/p&gt;

&lt;h2 id=&quot;social-functions&quot;&gt;3. Social functions&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;A number of social functions were dotted through the conference. As already mentioned there was the welcome BBQ on the Monday after the fieldtrips. On the Tuesday, I attended the Soil Science Australia AGM. Here we were introduced to the newly elected Federal Council. The new Council could best be described as youthful, and consists of John Bennett (President, USQ), Dan Brough (Vice President, QLD Govt), and Zoe Reed (Treasurer, ANU). The departure of several key members of the Federal Council, particularly of Executive officer Linda Bennison, leave some considerablly big shoes to fill for the future direction of the society. But the youthful team leading the society now will surely push it to new and unimagined places. I look forward to see where we are headed.&lt;/p&gt;

&lt;p&gt;Probably one of the highlights for the conference for me was an organised touch football competition that was held at the end of proceedings on the Wednesday. The concept of having a sporting context amongst colleagues and delegates is a stroke of brilliance. I think there were enough players to fill six teams. There was a USYD team (with a couple of drifters), plus a couple of QLD teams and other teams from NZ. There was even a crowd of onlookers taking in the spectacle. I don’t think any team was crowned a the ultimate victor, but it was just good all round fun and a chance to blow some steam off. I guess the pinnacle of the competition was a super match between Australia and New Zealand which abruptly ended when one of the Australian players from Queensland (sorry don’t know his name) dislocated his shoulder whilst scoring a try. This was obviously terrible news for the player injured, but as this was the only try scored in the match, Australia was the clear winner on the day. New Zealand will get the chance to atone in a rematch I guess at the next joint conference. It was much fun indeed.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/20161222queenstown/touch1.jpg&quot; alt=&quot;touch rugby&quot; /&gt;
    &lt;figcaption&gt;Figure 5. The Sydney Uni touch rugby team that took on the world in Queenstown. We did alright too.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The conference dinner was located at Skyline Restaurant which overlooks the town of Queenstown from several hundred metres up. The only way to get here is via a very steep gondola ride, where once to the top we were greeted which exceptional views and plenty of free beverages! There was plenty of good spirits on the night amongst the formalities of award giving and presentations. Our own Alex McBratney was awarded a Fellowship to Soil Science Australia. Other Australian award winners included Bob Gilkes (Honary Life Member) and Jock Churchman (JK Taylor Gold Medal in Soil Science). A couple of the really exceptional awards for the night was the announcement of the winners of the individual prizes in the soil judging contest. I think someone from the University of Wisconsin came in third, but our own USYD stars Bruce Tran and Rebecca McGirr came in second and first respectively. This would have been a massive achievement for them, and I was so happy for them too. All in all, the conference dinner night was super fun and enjoyable, although the body was paying for it the next morning a little bit.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/20161222queenstown/alex1.jpg&quot; alt=&quot;alex award&quot; /&gt;
    &lt;figcaption&gt;Figure 6. Alex McBratney collected a number of awards during 2016. Here he is recieving his Fellowship Award from Soil Science Australia and giving an entertaining acceptance speech.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;final-points&quot;&gt;Final Points&lt;/h3&gt;

&lt;p&gt;Much thanks goes to the New Zealand Soil Science Society for putting on a fabulous conference. Although the weather was a bit fickle, and at times pretty unforgiving (especially in December), Queenstown and the surrounding area is a beautiful place of the world. I would love to come visit here again sometime.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;/2016/12/queenstown&quot;&gt;Soil Science Conference, Queenstown, New Zealand&lt;/a&gt; was originally published by Brendan Malone at &lt;a href=&quot;&quot;&gt;Brendan Malone&lt;/a&gt; on December 22, 2016.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Website Launch]]></title>
 <link rel="alternate" type="text/html" href="/2016/11/website-launch" />
  <id>/2016/11/website-launch</id>
  <updated>2016-11-17T00:00:00-00:00</updated>
  <published>2016-11-17T00:00:00+11:00</published>
  
  <author>
    <name>Brendan Malone</name>
    <uri></uri>
    <email>malone.brendan1001@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;I have always had some interest or wonder into what and how websites are powered. Like where is all this content stored? And how is it all organised? Only recently these questions were more-or-less answered for me.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;This was when i undertook a short course on &lt;a href=&quot;https://www.codecademy.com/learn&quot;&gt;codecademy&lt;/a&gt;. It was definitely worth my while.&lt;/p&gt;

&lt;p&gt;So these are just some pointers that i found entirely useful for helping me develop and deploy a website on to the internet. As you can see, this website you are using is the outcome of my learning about this to date. It is by no means a finished product either.&lt;/p&gt;

&lt;p&gt;There are 3 main steps to achieve these ends.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate a static site&lt;/li&gt;
  &lt;li&gt;Deploy the site to the Internet&lt;/li&gt;
  &lt;li&gt;Give the website a custom domain name (optional step).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will just give a rather brief outline of these main steps but be aware that there are many little bumps and bungles to be encountered along the way as developing your own website will introduce you to problems specific to your use case. I have found though that a little intuition and iteration are the best means of finding the way through the issues that crop up. I have found that making a website is actually very fiddly and demands a lot of your time. I would not really describe myself as a computer literate either. More of a hack, but cobbling a few bits and pieces together of other peoples contributions has actually been the way i have floundered through to a result (a working website) that is mine and has my name all over it. It has my own personal brand, and that is what i wanted to achieve. I learned a lot in this process, so lets just get down to business.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-website&quot;&gt;1. Creating a Website&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Jekyll i have found is a really cool tool to quickly generate a website. Jekyll is a simple static site generator. Using Jekyll is a very common way of generating a “ready-to-publish static website” within seconds. You can learn more about &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; here.&lt;/p&gt;

&lt;p&gt;This post just demonstrates the process of using Jekyll to generate a static website quickly then focus on deploying it. You could use any sort of content you wish however and still follow the steps of deployment covered later on. Just make sure that your HTML is inside of a file called index.html. You can go ahead and create your own static site from scratch, but that is not for me. I am just going to do something simple, but bear in mind there is a vast array of &lt;a href=&quot;http://jekyllthemes.org/&quot;&gt;Jekyll Themes&lt;/a&gt; to tickle anyone’s fancy. The use of Github here is necessary though, as you can just select the theme of your choice, clone it onto your computer, then begin filling it with your own content.&lt;/p&gt;

&lt;p&gt;One catch is that Windows is not an officially supported platform for Jekyll. Consequently everything i do from here using Jekyll was development on Ubuntu 16.04. Before you install Jekyll, what you are going to need first is to install Ruby. More info on how to do this can be found at &lt;a href=&quot;https://www.brightbox.com/docs/ruby/ubuntu/&quot;&gt;brightbox&lt;/a&gt;. There maybe a few other software to install as well, so after you install Ruby you can then check out the &lt;a href=&quot;https://jekyllrb.com/docs/installation/&quot;&gt;Jekyll install instructions&lt;/a&gt;. Then in your terminal you can do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ sudo gem install jekyll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With Jekyll now installed we use Jekyll’s new command and specify a directory name. The directory will contain all of your site’s default content that can be customized later. For example, to generate a website in a directory called my-own-site, we can type:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ jekyll new my-own-site
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use Jekyll to view your site locally. On the web, a server hosts your site’s files and makes your website available for everyone to see. However, viewing a website locally means that you’re viewing the site on your own computer (hence the term “locally” or “local”). The site is not, however, available on the public Internet. Instead, your computer is acting as the server that hosts your site. You can view your site locally by using Jekyll’s serve command, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ jekyll serve
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command starts a local server that will server the files to your computer. The serve command will also come in handy when you want to preview changes you make to your site.&lt;/p&gt;

&lt;p&gt;The website that Jekyll generates offers a standard directory structure, as well as components that help speed up development. It’s important to understand Jekyll’s default directory structure and contents of your site. &lt;a href=&quot;https://jekyllrb.com/docs/structure/&quot;&gt;See here&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;deploy-website&quot;&gt;2. Deploy Website&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;There are many different ways to deploy a website to the public Internet. One of them uses GitHub Pages to deploy your website. GitHub Pages is a service offered by &lt;a href=&quot;https://github.com/&quot;&gt;GitHub&lt;/a&gt;. Specifically, GitHub Pages are public webpages that are hosted and published through GitHub. GitHub Pages offers extensive integration and support for Jekyll. By using both, we can benefit from:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Easy setup&lt;/li&gt;
  &lt;li&gt;Troubleshooting your site&lt;/li&gt;
  &lt;li&gt;Updating and maintaining your site&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously one caveat to successfully deploy your site, is that you will need a GitHub account.&lt;/p&gt;

&lt;p&gt;In order to publish your site using GitHub Pages, you’ll need to create a repository (repo) on GitHub. A GitHub repository is an online, central storage place where you can store files and all the versions of those files. We’ll use the repo create just before to store the contents of the basic website. A repo’s name must also follow GitHub Pages’ naming convention, otherwise the site will not publish at all. Specifically, the repo’s name must be in the following format:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;your-user-name.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the example above, you would replace your-user-name with your actual GitHub user name. Now we can upload the site that that was created (&lt;code&gt;new my-own-site&lt;/code&gt;) to GitHub. We’ll use Git to push (upload) the contents of your site’s directory to your new repo. To do so, we’ll first initialize a Git repository in the site’s directory. First, use the &lt;code&gt;cd&lt;/code&gt; command to navigate to the site’s directory. Once inside the &lt;code&gt;new my-own-site&lt;/code&gt; directory, initialize a Git repository with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ git init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, Git needs to know what repo will store your site’s content. In this case, the repo will be the one you created on GitHub earlier. To specify the repo using Git, we’ll have to add the remote and label it as the origin.&lt;/p&gt;

&lt;p&gt;The remote is the URL of the repo that will store your site’s contents. The origin is an alias for the remote. You can think of an alias as an abbreviation or a substitute name. This means that instead of having to always type the lengthy remote URL over and over again, you can simply refer to it as origin later on. In the terminal, you can add the remote with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ git remote add origin https://github.com/your-user-name/your-user-name.github.io.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Git also needs to know exactly which files should be pushed to your repo. In this case, we want to push all of your site’s content to the repo. This means we will do the following two things (in order):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add all of your site’s content to the Git staging area&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ git add .
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;Save your changes using Git’s commit command and the following commit message:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ git commit -m &quot;Save my work&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now use Git to help deploy the site. This time, we’ll use Git’s push command and push the contents of your site up to the repo using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;$ git push -u origin master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The site will now be published on the public Internet. You can now navigate to your newly published website in your preferred browser. The URL for your GitHub Pages site is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;your-user-name.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where your-user-name is your actual GitHub username.&lt;/p&gt;

&lt;h2 id=&quot;give-the-website-a-custom-domain-name&quot;&gt;2. Give the website a custom domain name&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;By now we have deployed the site and GitHub Pages has assigned it a default URL, or domain name. The next step is to purchase your own custom domain name and assign it to your GitHub Pages website. This will result in you being able to access your site using both your new domain name and your default GitHub Pages domain name.&lt;/p&gt;

&lt;p&gt;Before you choose a custom domain name, it’s important to first understand what domain names actually are. Domain names are human-friendly names that identify servers on the Internet. A global system known as the Domain Name System (DNS) is used for storing which domain names correspond to which servers. For example, Github’s’s domain name is &lt;code&gt;https://github.com&lt;/code&gt;. When you type the domain name into your browser, your computer asks the DNS to identify which servers should receive the request in order to load the website.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This part of website deployment is optional. Nevertheless, you can follow through the steps required to purchase a custom domain name and assign it to your GitHub Pages site. From my own experience a custom domain name costs about $15 AUS.&lt;/p&gt;

&lt;p&gt;Often, the most time consuming part of buying a domain name is actually deciding what you’d like it to be. Be aware that not all domain names are available; many have already been claimed by others. Here we are going to use Amazon Web Services (AWS) to purchase your custom domain. &lt;a href=&quot;https://aws.amazon.com/&quot;&gt;AWS&lt;/a&gt; is an industry standard suite of web infrastructure services used frequently by developers. The specific service we’re going to use to purchase your domain name is called &lt;code&gt;Route 53&lt;/code&gt;. To begin the process you will need to establish an account with AWS if you do not have one already. Creating an AWS account is free - there are no required purchases. The only purchase that you’ll (optionally) make will be the purchase of your custom domain name.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Instructions&lt;/strong&gt;
After logging into your account, you’ll land on the AWS console. The console displays the many different services. Here we are going to focus on a service called “Route 53,” under the “Networking” category. Route 53 can be used to purchase domain names and create DNS records. Once in here, click on the “Get Started Now” button under the “Domain Registration” section. On the next page, locate the two buttons at the top of the page. Click on the button titled “Register Domain”.&lt;/p&gt;

&lt;p&gt;Now it’s time to select a domain name and make sure it’s available. Route 53 allows you to search the availability of a domain name you have in mind. It also offers many suffixes, like &lt;code&gt;.com&lt;/code&gt;, &lt;code&gt;.io&lt;/code&gt;, &lt;code&gt;.me&lt;/code&gt;, and &lt;code&gt;.soil&lt;/code&gt;. If the domain name you want is unavailable as a &lt;code&gt;.com&lt;/code&gt; for example, you can try using a different suffix. The suffixes of domain names are known as top-level domains (TLDs). Different TLDs cost different annual prices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;.com&lt;/code&gt; domains are the most popular and are therefore generally unavailable (or expensive).&lt;/p&gt;

&lt;p&gt;With your new domain name established,you might notice that it doesn’t work yet - you can’t visit it in your web browser. We have to connect it to your GitHub Pages website first. There are two steps required:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inform GitHub of the new domain name we’ll be using (the one you purchased)&lt;/li&gt;
  &lt;li&gt;Set up DNS records in Route 53 that direct to GitHub&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To perform these steps, open GitHub and access the repo you created earlier titled &lt;code&gt;your-user-name.github.io&lt;/code&gt;. Click the “New file” button. Name the new file &lt;code&gt;CNAME&lt;/code&gt;. Do not add a file name extension. In the file, on line 1, type the custom domain name you just purchased in the following format: &lt;code&gt;yourcustomdomain.com&lt;/code&gt;. You may have purchased a domain name with a TLD other than .com. In that case, make sure to use that TLD when creating the &lt;code&gt;CNAME&lt;/code&gt; file. Commit the new file. Under the title of the repo, click on “Settings.” Scroll down to the section titled “GitHub Pages” and confirm that there is a message similar to the following: Your site is published at &lt;code&gt;http://yourcustomdomain.com&lt;/code&gt;. Try navigating to your website in your browser using your new domain name. It still doesn’t work! Time to move onto the next step.&lt;/p&gt;

&lt;p&gt;The new &lt;code&gt;CNAME&lt;/code&gt; file in your repo informs GitHub that you’re assigning a new custom domain name to your GitHub Pages site. What we do now is let the rest of the Internet know that we want to associate the custom domain name with your GitHub Pages site. We can do this by creating DNS records, which are globally accessible records that map domain names to servers. The DNS records are created inside of a Hosted Zone in Route 53. A Hosted Zone is essentially a group of DNS records for a single domain.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Instructions&lt;/strong&gt;
In your AWS account Access Route 53 again. On the left side of the page, click on the title that says “Hosted Zones.” Notice that you have a Hosted Zone for your new domain name. Click on it to open it.&lt;/p&gt;

&lt;p&gt;Domain names are associated with the correct DNS records by setting the domain name’s name servers. After a domain name is typed into a browser, the computer first retrieves the name servers that correspond to that domain name. The name servers are important because they’re responsible with providing the computer with other important information (in the form of DNS records) associated with the domain name. Setting your domain’s name servers is important. The DNS is a global system, which means that anyone can create DNS records. We must verify that the DNS records we create were actually created by the owner of the domain name (in this case, you). By doing this, the owner of a domain name ensures that only they have exclusive control over their domain’s DNS records.&lt;/p&gt;

&lt;p&gt;Notice that the Hosted Zone for your domain name already has an &lt;code&gt;NS (Name server)&lt;/code&gt; record. This record contains four values. These are the Hosted Zone’s unique name servers. Take note of these values and copy them down somewhere. On the left hand side, under “Domains,” click on “Registered domains.” Then, click on your domain name. On the right hand side of the page, locate the section titled “Name Servers.” Notice that these are the same name servers that your Hosted Zone’s &lt;code&gt;NS&lt;/code&gt; record contained. Route 53 did this for you automatically.&lt;/p&gt;

&lt;p&gt;Now that your domain name is associated with the correct name servers, it’s time to create some additional DNS records within the Hosted Zone. The records that we’ll create will be used by the name servers to help locate your site when a computer wants to load it. Specifically, the name servers will be responsible for providing that computer with important information stored in the records.&lt;/p&gt;

&lt;p&gt;There are several different types of DNS records. We’re going to start by creating an &lt;code&gt;A&lt;/code&gt; record, which stands for Address record. An &lt;code&gt;A&lt;/code&gt; record directs a domain name to an IP address. This record will associate our new custom domain name with Github’s servers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Instructions&lt;/strong&gt;
Inside of your Hosted Zone, click on the button at the top labeled “Create Record Set.” A form will appear to the right. Leave the “Name:” field blank. Set the “Type:” field to A - IPv4 address.
Leave the “TTL (Seconds)” value at the default of 300. In the “Value” text box, enter the following IP addresses (keep them on separate lines):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;192.30.252.153
192.30.252.154
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These IP addresses belong to GitHub. We are specifying that when your custom domain name is requested, the DNS should direct the request to GitHub. Read more information about this &lt;a href=&quot;https://help.github.com/articles/setting-up-an-apex-domain/#configuring-a-records-with-your-dns-provider&quot;&gt;here&lt;/a&gt;. Click the “Save Record Set” button at the bottom of the form.&lt;/p&gt;

&lt;p&gt;When setting up a website, it’s also conventional to also set up a &lt;code&gt;www&lt;/code&gt; subdomain. &lt;code&gt;www&lt;/code&gt; stands for &lt;code&gt;world wide web&lt;/code&gt;. Subdomains are part of a main (or root) domain. For example, &lt;code&gt;www.yourcustomdomain.com&lt;/code&gt; is a subdomain of the &lt;code&gt;yourcustomdomain.com&lt;/code&gt; root domain. We can set up a subdomain using a &lt;code&gt;CNAME&lt;/code&gt; record, which stands for Canonical Name. A &lt;code&gt;CNAME&lt;/code&gt; record specifies that a domain name will be used as an alias, or substitute, for the true (canonical) domain name.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Instructions&lt;/strong&gt;
Inside of your Hosted Zone, click on the button at the top labeled “Create Record Set”. A form will appear to the right. In the “Name:” field, enter only &lt;code&gt;www&lt;/code&gt;. Set the “Type: “ field to &lt;code&gt;CNAME - Canonical name&lt;/code&gt;. This step sets up the subdomain. In the &lt;code&gt;Value&lt;/code&gt; text box, enter the domain name that GitHub assigned to you earlier (the canonical domain name:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;your-user-name.github.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Click the “Save Record Set” button at the bottom of the form.&lt;/p&gt;

&lt;p&gt;W have now created two DNS records: an &lt;code&gt;A&lt;/code&gt; record for &lt;code&gt;yourcustomdomain.com&lt;/code&gt; and a &lt;code&gt;CNAME&lt;/code&gt; record for &lt;code&gt;www.yourcustomdomain.com&lt;/code&gt;. Let’s make sure they both work.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important Instructions&lt;/strong&gt;
Try opening your website using your root domain in the web browser. You should see your new GitHub Pages site. Try opening your website using your &lt;code&gt;www&lt;/code&gt; subdomain in the web browser. You should see your new GitHub Pages site.&lt;/p&gt;

&lt;p&gt;Success! The website should now display in the browser when you navigate to your custom domain name.&lt;/p&gt;

&lt;h3 id=&quot;final-points&quot;&gt;Final Points&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We have used Jekyll to create a static website, then used Github pages to host it on the internet. We then set up a custom domain so that the website can be widely seen on the Internet.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is much to learn in terms of building content for your website. You should also look into the many Jekyll content templates that are available.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I find building Jekyll website content to be relatively easy as you only have to learn Markdown. Jekyll does the amazing job of converting it all in html. I have found though that even with this efficiency, building content can take an age.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;code&gt;jekyll serve&lt;/code&gt; command is super useful. You can make changes and quickly check the result by deploying the site locally.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;


    &lt;p&gt;&lt;a href=&quot;/2016/11/website-launch&quot;&gt;Website Launch&lt;/a&gt; was originally published by Brendan Malone at &lt;a href=&quot;&quot;&gt;Brendan Malone&lt;/a&gt; on November 17, 2016.&lt;/p&gt;
  </content>
</entry>

</feed>
